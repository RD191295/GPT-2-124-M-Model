<!-- Banner (replace with your own image if available) -->
<p align="center">
  <img src="https://raw.githubusercontent.com/RD191295/GPT-2-124-M-Model/main/assets/banner.png" alt="GPT-2 Banner" width="80%">
</p>

<h1 align="center">ğŸ¤– GPT-2 124M â€” From Scratch</h1>

<p align="center">
  <b>A clean PyTorch implementation of GPT-2 (124M parameters)</b><br/>
</p>

<p align="center">
  <a href="https://github.com/RD191295/GPT-2-124-M-Model/stargazers">
    <img src="https://img.shields.io/github/stars/RD191295/GPT-2-124-M-Model?style=social" alt="Stars"/>
  </a>
  <a href="https://github.com/RD191295/GPT-2-124-M-Model/issues">
    <img src="https://img.shields.io/github/issues/RD191295/GPT-2-124-M-Model" alt="Issues"/>
  </a>
  <a href="https://github.com/RD191295/GPT-2-124-M-Model/blob/main/LICENSE">
    <img src="https://img.shields.io/github/license/RD191295/GPT-2-124-M-Model" alt="License"/>
  </a>
</p>

---

## ğŸ“‘ Table of Contents
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ› ï¸ Training (Planned)](#ï¸-training-planned)
- [ğŸŒŸ Roadmap](#-roadmap)
- [ğŸ“ Example Generation (Planned)](#-example-generation-planned)
- [ğŸ™ Acknowledgments](#-acknowledgments)
- [ğŸ“– References](#-references)
- [ğŸ“œ License](#-license)
- [ğŸ“– Citation](#-citation)
- [ğŸ¤ Contributing](#-contributing)

---

## âœ¨ Features

- âš¡ **Faithful GPT-2 Architecture** â€” Transformer blocks, causal self-attention, GELU feed-forward
- ğŸ“š **Educational** â€” detailed NumPy-style docstrings for every module
- ğŸ”§ **Modular Design** â€” easy to customize layers, heads, embedding size
- ğŸ§© **Pre-Norm Transformer** â€” stable training with layer norm before sublayers
- ğŸ’¾ **Checkpoint-Friendly** â€” model is structured for saving/loading easily

---

## ğŸ—ï¸ Architecture

<p align="center">
  <img src="https://raw.githubusercontent.com/RD191295/GPT-2-124-M-Model/main/assets/architecture.png" alt="GPT2 Architecture" width="70%">
</p>

The model consists of:
- Token + positional embeddings  
- A stack of Transformer blocks  
- Final layer normalization  
- Linear output head â†’ vocabulary logits  

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install

```bash
git clone https://github.com/RD191295/GPT-2-124-M-Model.git
cd GPT-2-124-M-Model
pip install -r requirements.txt
```

### 2ï¸âƒ£ Use the Model

```python 
import torch
from model import GPT2Model

model = GPT2Model(
    vocab_size=50257,
    context_length=1024,
    emb_dim=768,
    n_heads=12,
    n_layers=12,
    dropout=0.1,
    qkv_bias=True
)

# dummy input
input_ids = torch.randint(0, 50257, (2, 128))
logits = model(input_ids)
print(logits.shape)  # (2, 128, 50257)
```

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ model.py         # TransformerBlock & GPT2Model
â”œâ”€â”€ trainer.py       # (WIP) Training loop
â”œâ”€â”€ utils.py         # (optional) helper functions
â”œâ”€â”€ assets/          # images for README
â”œâ”€â”€ requirements.txt # dependencies
â””â”€â”€ README.md
```

--

## ğŸ› ï¸ Training (Planned)

âš ï¸ Training loop (`trainer.py`) is still work-in-progress. Features planned:

- [ ] CrossEntropy loss with shifted labels  
- [ ] AdamW optimizer + weight decay  
- [ ] Linear LR scheduler with warmup  
- [ ] Checkpoint saving/loading  
- [ ] Gradient clipping  

ğŸ‘‰ The `trainer.py` file will cover this in detail soon!

---

## ğŸŒŸ Roadmap

- [ ] Finish training loop (`trainer.py`)  
- [ ] Add tokenizer integration (`GPT2Tokenizer` from tiktoken)  
- [ ] Implement text generation (greedy, top-k, top-p sampling)  
- [ ] Release pretrained checkpoints  

---

## ğŸ™ Acknowledgments

- [OpenAI GPT-2 (2019)](https://openai.com/research/language-unsupervised)  
- [Andrej Karpathyâ€™s nanoGPT](https://github.com/karpathy/nanoGPT)  
- [Build LLM From Scratch-Vizura](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)
- [Gen AI Course By Sudhanshu](https://euron.one/course/generative-ai-with-nlp-agentic-ai-and-fine-tuning)
---

## ğŸ“– References

- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).  
  *Language Models are Unsupervised Multitask Learners*.  
  [[Paper PDF]](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [[OpenAI Blog]](https://openai.com/research/language-unsupervised)

---

## ğŸ“œ License
This project is licensed under the MIT License

---

ğŸ¤ Contributing

Contributions are welcome! ğŸ‰
If youâ€™d like to improve the code, add features, or fix bugs:

- Fork the repo
- Create a new branch (feature-xyz)
- Commit your changes
- Submit a Pull Request

---

## ğŸ“– Citation

If you use this codebase or find it helpful, please consider citing the original GPT-2 paper:

```bibtex
@article{radford2019language,
  title     = {Language Models are Unsupervised Multitask Learners},
  author    = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year      = {2019},
  journal   = {OpenAI Blog},
  volume    = {1},
  number    = {8},
  url       = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}
```

ğŸ‘‰ Please also cite this repository if you build upon it or use it in your research:

```bibtex
@misc{dalsaniya2025gpt2scratch,
  author       = {Raj Dalsaniya},
  title        = {GPT-2 124M from Scratch (PyTorch Implementation)},
  year         = {2025},
  howpublished = {\url{https://github.com/RD191295/GPT-2-124-M-Model}},
  note         = {GitHub repository}
}
```

--

ğŸ’¡ If you like this repo, donâ€™t forget to â­ star it on GitHub!